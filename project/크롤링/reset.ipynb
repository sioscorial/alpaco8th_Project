{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/2905 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping for brand: DISCOVERYEXPEDITION from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping DISCOVERYEXPEDITION - Page 1:   0%|          | 0/90 [00:09<?, ?it/s]\n",
      "Overall Progress:   0%|          | 0/2905 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 189\u001b[0m\n\u001b[0;32m    186\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 183\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 새 브랜드는 첫 페이지부터 시작\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting scraping for brand: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_page\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 183\u001b[0m     \u001b[43mscrape_brand_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 다음 브랜드를 위해 시작 페이지를 리셋\u001b[39;00m\n\u001b[0;32m    186\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[5], line 165\u001b[0m, in \u001b[0;36mscrape_brand_data\u001b[1;34m(driver, brand, start_page, end_page)\u001b[0m\n\u001b[0;32m    163\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m91\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 165\u001b[0m     item_details \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_product_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiv_nums\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item_details:\n\u001b[0;32m    167\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(item_details)\n",
      "Cell \u001b[1;32mIn[5], line 110\u001b[0m, in \u001b[0;36mscrape_product_info\u001b[1;34m(driver, brand, item, div_nums)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div_num \u001b[38;5;129;01min\u001b[39;00m div_nums:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         item_price_element \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisibility_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//*[@id=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearchList\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]/li[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mitem\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]/div[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdiv_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]/div[2]/p[3]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m         item_price \u001b[38;5;241m=\u001b[39m item_price_element\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_window_size(1800, 1800)\n",
    "    return driver\n",
    "\n",
    "def load_progress(filename='Items_Info.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        if df.empty:\n",
    "            return None, 1\n",
    "        # 마지막으로 데이터가 저장된 브랜드와 페이지 번호를 가져옵니다.\n",
    "        last_brand = df['Brand'].iloc[-1]\n",
    "        last_page = df['Page_Number'].iloc[-1]\n",
    "        return last_brand, last_page\n",
    "    except FileNotFoundError:\n",
    "        return None, 1\n",
    "\n",
    "def scrape_brand_data(driver, brand, start_page=1, end_page=100, save_interval=10):\n",
    "    all_data = []  # 모든 페이지의 데이터를 저장할 리스트\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f'https://www.musinsa.com/brands/{brand}?page={page_num}&size=90'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        if is_page_empty(driver):\n",
    "            print(f\"No items found for brand {brand} on page {page_num}.\")\n",
    "            break\n",
    "\n",
    "        data = []\n",
    "        for item in tqdm(range(1, 91), desc=f\"Scraping {brand} - Page {page_num}\"):\n",
    "            item_details = scrape_product_info(driver, brand, item, div_nums)\n",
    "            if item_details:\n",
    "                data.append(item_details)\n",
    "                all_data.append(item_details)  # 현재 페이지의 데이터를 전체 데이터에 추가\n",
    "\n",
    "        # 설정된 간격마다 또는 마지막 페이지에서 데이터 저장\n",
    "        if page_num % save_interval == 0 or page_num == end_page:\n",
    "            save_data(all_data, brand, page_num)\n",
    "            all_data.clear()  # 저장 후 전체 데이터 리스트 초기화\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_progress(filename='completed_brands.csv'):\n",
    "#     try:\n",
    "#         df = pd.read_csv(filename)\n",
    "#         if df.empty:\n",
    "#             return None, 1\n",
    "#         last_row = df.iloc[-1]\n",
    "#         brand = last_row['Brand']\n",
    "#         last_completed_page = last_row['Page_Number']\n",
    "#         return brand, last_completed_page\n",
    "#     except FileNotFoundError:\n",
    "#         return None, 1\n",
    "\n",
    "# def save_progress(completed_brands, filename='completed_brands.csv'):\n",
    "#     pd.DataFrame(completed_brands, columns=['Brand', 'Page_Number']).to_csv(filename, index=False)\n",
    "\n",
    "# def save_data(data, brand, page_num, brand_page_info='Items_Info.csv'):\n",
    "#     if data:\n",
    "#         new_data = pd.DataFrame(data)\n",
    "#         brand_page_info = f\"./Datasets/{brand}_page_{page_num}.csv\"\n",
    "#         if not os.path.exists(\"./Datasets\"):\n",
    "#             os.makedirs(\"./Datasets\")\n",
    "#         if not os.path.exists(brand_page_info):\n",
    "#             new_data.to_csv(brand_page_info, index=False)\n",
    "#         else:\n",
    "#             new_data.to_csv(brand_page_info, mode='a', header=False, index=False)\n",
    "#     else:\n",
    "#         print(\"No data to save.\")\n",
    "\n",
    "\n",
    "### 하나의 파일로 맨마지막 브랜드 + page 를 가져와서 그 후 부터 다시 시작\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_page_empty(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchList\"]/li[1]')))\n",
    "        return False\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            no_items_text = driver.find_element(By.XPATH, '//*[@id=\"result-none-area\"]/p').text\n",
    "            if \"등록된 상품이 없습니다\" in no_items_text:\n",
    "                return True\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "\n",
    "div_nums = [1,2,3,4]\n",
    "\n",
    "def scrape_product_info(driver, brand, item, div_nums):\n",
    "    item_price = None\n",
    "    item_details = {\n",
    "        \"brand\": None,\n",
    "        \"item_category\": None,\n",
    "        \"item_img\": None,\n",
    "        \"item_name\": None,\n",
    "        \"item_num\": None,\n",
    "        \"item_brand\": None,\n",
    "        \"item_price\": None\n",
    "    }\n",
    "\n",
    "    for div_num in div_nums:\n",
    "        try:\n",
    "            item_price_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, f'//*[@id=\"searchList\"]/li[{item}]/div[{div_num}]/div[2]/p[3]')))\n",
    "            item_price = item_price_element.text\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            print(\"가격 못찾음\")\n",
    "            pass\n",
    "    \n",
    "    for div_num in div_nums:\n",
    "        try:\n",
    "            item_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, f'//*[@id=\"searchList\"]/li[{item}]/div[{div_num}]/div[2]/p[2]/a')))\n",
    "            driver.execute_script(\"arguments[0].click();\", item_element)\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            print(\"클릭못찾음\")\n",
    "            pass\n",
    "   \n",
    "    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[1]/div[1]/div[1]/a[1]')))\n",
    "\n",
    "    for div_num in div_nums:\n",
    "        try:\n",
    "            item_details[\"item_category\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[1]/div[1]/a[1]').text\n",
    "            item_details[\"item_img\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[2]/div[1]/div[1]/div/img').get_attribute('src')\n",
    "            item_details[\"item_name\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[1]/div[3]/div[2]/h3').text\n",
    "            item_details[\"item_num\"] = driver.find_element(By.XPATH, f'//*[@id=\"root\"]/div[1]/div[2]/div[2]/div[{div_num}]/ul/li[1]/div[2]/span[2]').text\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            print(\"카테고리,이미지,이름,제품번호중 못찾음\")\n",
    "            pass\n",
    "\n",
    "    for div_num in div_nums:\n",
    "        try:\n",
    "            item_details[\"item_brand\"] = driver.find_element(By.XPATH, f'//*[@id=\"root\"]/div[1]/div[2]/div[2]/div[{div_num}]/ul/li[1]/div[2]/a').text\n",
    "            item_details[\"item_price\"] = item_price\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            print(\"브랜드나 가격 오류\")\n",
    "            driver.quit()\n",
    "        \n",
    "\n",
    "    driver.back()\n",
    "    return item_details\n",
    "\n",
    "\n",
    "\n",
    "def scrape_brand_data(driver, brand, start_page=1, end_page=100):\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f'https://www.musinsa.com/brands/{brand}?page={page_num}&size=90'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        if is_page_empty(driver):\n",
    "            print(f\"No items found for brand {brand} on page {page_num}.\")\n",
    "            break\n",
    "\n",
    "        data = []\n",
    "        for item in tqdm(range(1, 91), desc=f\"Scraping {brand} - Page {page_num}\"):\n",
    "            item_details = scrape_product_info(driver, brand, item, div_nums)\n",
    "            if item_details:\n",
    "                data.append(item_details)\n",
    "\n",
    "        save_data(data, brand, page_num)\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    df = pd.read_csv('Brand_Name.csv')\n",
    "    brand_names = df['Brand_ENG'].tolist()\n",
    "    last_completed_brand, start_page = load_progress()\n",
    "\n",
    "    for brand in tqdm(brand_names, desc=\"Overall Progress\"):\n",
    "        if brand == last_completed_brand:\n",
    "            start_page += 1  # 마지막으로 완료된 브랜드의 다음 페이지부터 시작\n",
    "        else:\n",
    "            start_page = 1  # 새 브랜드는 첫 페이지부터 시작\n",
    "        print(f\"Starting scraping for brand: {brand} from page {start_page}\")\n",
    "        scrape_brand_data(driver, brand, start_page=start_page)\n",
    "        start_page = 1  # 다음 브랜드를 위해 시작 페이지를 리셋\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/2891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping for brand: SCULPTOR from page 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping SCULPTOR - Page 4: 100%|██████████| 90/90 [04:05<00:00,  2.73s/it]\n",
      "Scraping SCULPTOR - Page 5: 100%|██████████| 90/90 [04:06<00:00,  2.74s/it]\n",
      "Scraping SCULPTOR - Page 6: 100%|██████████| 90/90 [04:03<00:00,  2.71s/it]\n",
      "Scraping SCULPTOR - Page 7: 100%|██████████| 90/90 [04:05<00:00,  2.73s/it]\n",
      "Overall Progress:   0%|          | 1/2891 [16:38<801:25:13, 998.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand SCULPTOR on page 8.\n",
      "Starting scraping for brand: GLOWNY from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping GLOWNY - Page 1: 100%|██████████| 90/90 [04:01<00:00,  2.68s/it]\n",
      "Scraping GLOWNY - Page 2: 100%|██████████| 90/90 [04:07<00:00,  2.75s/it]\n",
      "Scraping GLOWNY - Page 3: 100%|██████████| 90/90 [04:13<00:00,  2.82s/it]\n",
      "Scraping GLOWNY - Page 4: 100%|██████████| 90/90 [04:08<00:00,  2.76s/it]\n",
      "Overall Progress:   0%|          | 2/2891 [33:24<804:38:06, 1002.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand GLOWNY on page 5.\n",
      "Starting scraping for brand: PLACESTUDIO from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping PLACESTUDIO - Page 1: 100%|██████████| 90/90 [04:28<00:00,  2.98s/it]\n",
      "Scraping PLACESTUDIO - Page 2: 100%|██████████| 90/90 [04:30<00:00,  3.00s/it]\n",
      "Scraping PLACESTUDIO - Page 3: 100%|██████████| 90/90 [04:32<00:00,  3.03s/it]\n",
      "Scraping PLACESTUDIO - Page 4: 100%|██████████| 90/90 [04:34<00:00,  3.05s/it]\n",
      "Overall Progress:   0%|          | 3/2891 [51:44<840:11:24, 1047.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand PLACESTUDIO on page 5.\n",
      "Starting scraping for brand: SIGNATURE from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping SIGNATURE - Page 1: 100%|██████████| 90/90 [04:47<00:00,  3.20s/it]\n",
      "Scraping SIGNATURE - Page 2: 100%|██████████| 90/90 [04:45<00:00,  3.18s/it]\n",
      "Scraping SIGNATURE - Page 3: 100%|██████████| 90/90 [04:43<00:00,  3.15s/it]\n",
      "Overall Progress:   0%|          | 4/2891 [1:06:15<784:04:55, 977.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand SIGNATURE on page 4.\n",
      "Starting scraping for brand: FABREGAT from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping FABREGAT - Page 1: 100%|██████████| 90/90 [04:52<00:00,  3.25s/it]\n",
      "Scraping FABREGAT - Page 2: 100%|██████████| 90/90 [04:48<00:00,  3.20s/it]\n",
      "Overall Progress:   0%|          | 5/2891 [1:16:10<673:09:45, 839.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand FABREGAT on page 3.\n",
      "Starting scraping for brand: MARDIMERCREDI from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping MARDIMERCREDI - Page 1: 100%|██████████| 90/90 [05:00<00:00,  3.34s/it]\n",
      "Scraping MARDIMERCREDI - Page 2: 100%|██████████| 90/90 [05:04<00:00,  3.39s/it]\n",
      "Scraping MARDIMERCREDI - Page 3: 100%|██████████| 90/90 [05:03<00:00,  3.37s/it]\n",
      "Scraping MARDIMERCREDI - Page 4: 100%|██████████| 90/90 [05:02<00:00,  3.37s/it]\n",
      "Scraping MARDIMERCREDI - Page 5: 100%|██████████| 90/90 [05:13<00:00,  3.48s/it]\n",
      "Scraping MARDIMERCREDI - Page 6: 100%|██████████| 90/90 [05:18<00:00,  3.54s/it]\n",
      "Scraping MARDIMERCREDI - Page 7: 100%|██████████| 90/90 [05:22<00:00,  3.58s/it]\n",
      "Scraping MARDIMERCREDI - Page 8: 100%|██████████| 90/90 [05:24<00:00,  3.60s/it]\n",
      "Overall Progress:   0%|          | 6/2891 [1:58:06<1129:37:28, 1409.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand MARDIMERCREDI on page 9.\n",
      "Starting scraping for brand: SLOWACID from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping SLOWACID - Page 1: 100%|██████████| 90/90 [05:49<00:00,  3.88s/it]\n",
      "Scraping SLOWACID - Page 2: 100%|██████████| 90/90 [05:50<00:00,  3.89s/it]\n",
      "Scraping SLOWACID - Page 3: 100%|██████████| 90/90 [05:46<00:00,  3.85s/it]\n",
      "Scraping SLOWACID - Page 4: 100%|██████████| 90/90 [05:44<00:00,  3.83s/it]\n",
      "Overall Progress:   0%|          | 7/2891 [2:21:36<1129:17:17, 1409.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for brand SLOWACID on page 5.\n",
      "Starting scraping for brand: BROWNBREATH from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping BROWNBREATH - Page 1: 100%|██████████| 90/90 [05:57<00:00,  3.97s/it]\n",
      "Scraping BROWNBREATH - Page 2: 100%|██████████| 90/90 [06:03<00:00,  4.04s/it]\n",
      "Scraping BROWNBREATH - Page 3: 100%|██████████| 90/90 [06:13<00:00,  4.15s/it]\n",
      "Scraping BROWNBREATH - Page 4: 100%|██████████| 90/90 [06:13<00:00,  4.15s/it]\n",
      "Overall Progress:   0%|          | 7/2891 [2:46:17<1141:50:49, 1425.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 177\u001b[0m\n\u001b[0;32m    171\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 168\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 새 브랜드는 첫 페이지부터 시작\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting scraping for brand: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_page\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mscrape_brand_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 다음 브랜드를 위해 시작 페이지를 리셋\u001b[39;00m\n\u001b[0;32m    171\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[10], line 136\u001b[0m, in \u001b[0;36mscrape_brand_data\u001b[1;34m(driver, brand, start_page, end_page)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_page, end_page \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    135\u001b[0m     page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.musinsa.com/brands/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&size=90\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 136\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_page_empty(driver):\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo items found for brand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    137\u001b[0m         method,\n\u001b[0;32m    138\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\http\\client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\http\\client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\http\\client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ex1\\anaconda3\\envs\\alpaco0304\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_window_size(1800, 1800)\n",
    "    return driver\n",
    "\n",
    "def load_progress(filename='completed_brands.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        if df.empty:\n",
    "            return None, 1\n",
    "        last_row = df.iloc[-1]\n",
    "        brand = last_row['Brand']\n",
    "        last_completed_page = last_row['Page_Number']\n",
    "        return brand, last_completed_page\n",
    "    except FileNotFoundError:\n",
    "        return None, 1\n",
    "\n",
    "def save_progress(completed_brands, filename='completed_brands.csv'):\n",
    "    pd.DataFrame(completed_brands, columns=['Brand', 'Page_Number']).to_csv(filename, index=False)\n",
    "\n",
    "def is_page_empty(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchList\"]/li[1]')))\n",
    "        return False\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            no_items_text = driver.find_element(By.XPATH, '//*[@id=\"result-none-area\"]/p').text\n",
    "            if \"등록된 상품이 없습니다\" in no_items_text:\n",
    "                return True\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrape_product_info(driver, brand ,item):\n",
    "    item_price = None\n",
    "    item_details = {\n",
    "        \"brand\": brand,\n",
    "        \"item_category\": None,\n",
    "        \"item_img\": None,\n",
    "        \"item_name\": None,\n",
    "        \"item_num\": None,\n",
    "        \"item_brand\": None,\n",
    "        \"item_price\": None\n",
    "    }\n",
    "    time.sleep(0.5)\n",
    "    for div_num in range(4,1,-1):\n",
    "        try:\n",
    "            \n",
    "            item_price_element =driver.find_element(By.XPATH, f'//*[@id=\"searchList\"]/li[{item}]/div[{div_num}]/div[2]/p[3]')\n",
    "            item_price = item_price_element.text\n",
    "            \n",
    "           \n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "    time.sleep(0.5)\n",
    "    for div_num in range(1,5):\n",
    "        try:\n",
    "    \n",
    "            item_element = driver.find_element(By.XPATH, f'//*[@id=\"searchList\"]/li[{item}]/div[{div_num}]/div[2]/p[2]/a')\n",
    "            driver.execute_script(\"arguments[0].click();\", item_element)\n",
    "            \n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "   \n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    for num in range(1,5):\n",
    "        try:\n",
    "            item_details[\"item_category\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[1]/div[1]/a[1]').text\n",
    "            \n",
    "            item_details[\"item_img\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[2]/div[1]/div[1]/div/img').get_attribute('src')\n",
    "            \n",
    "            item_details[\"item_name\"] = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div[1]/div[3]/div[2]/h3').text\n",
    "            \n",
    "            item_details[\"item_num\"] = driver.find_element(By.XPATH, f'//*[@id=\"root\"]/div[1]/div[2]/div[2]/div[{num}]/ul/li[1]/div[2]/span[2]').text\n",
    "            \n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "\n",
    "    for div_num in range(1,5):\n",
    "        try:\n",
    "            item_details[\"item_brand\"] = driver.find_element(By.XPATH, f'//*[@id=\"root\"]/div[1]/div[2]/div[2]/div[{div_num}]/ul/li[1]/div[2]/a').text\n",
    "            item_details[\"item_price\"] = item_price\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "\n",
    "    driver.back()\n",
    "    \n",
    "    return item_details\n",
    "\n",
    "def save_data(data, brand, page_num, brand_page_info='Items_Info.csv'):\n",
    "    if data:\n",
    "        new_data = pd.DataFrame(data)\n",
    "        brand_page_info = f\"./Datasets/{brand}_page_{page_num}.csv\"\n",
    "        if not os.path.exists(\"./Datasets\"):\n",
    "            os.makedirs(\"./Datasets\")\n",
    "        if os.path.exists(brand_page_info):\n",
    "            # 파일이 존재하면, 이미 저장된 데이터를 확인\n",
    "            existing_data = pd.read_csv(brand_page_info)\n",
    "            if not existing_data.empty:\n",
    "                # 이미 저장된 데이터가 있다면, 새 데이터를 추가하지 않음\n",
    "                return\n",
    "        # 새 데이터를 파일에 저장\n",
    "        new_data.to_csv(brand_page_info, mode='a', header=not os.path.exists(brand_page_info), index=False)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n",
    "def scrape_brand_data(driver, brand, start_page=1, end_page=100):\n",
    "    completed_brands = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f'https://www.musinsa.com/brands/{brand}?page={page_num}&size=90'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        if is_page_empty(driver):\n",
    "            print(f\"No items found for brand {brand} on page {page_num}.\")\n",
    "            break\n",
    "\n",
    "        data = []\n",
    "        for item in tqdm(range(1, 91), desc=f\"Scraping {brand} - Page {page_num}\"):\n",
    "           \n",
    "            item_details = scrape_product_info(driver, brand, item)\n",
    "            if item_details:\n",
    "                data.append(item_details)\n",
    "            else:\n",
    "                print(f\"No item found at position {item} for brand {brand} on page {page_num}.\")\n",
    "\n",
    "        save_data(data, brand, page_num)\n",
    "        data.clear()\n",
    "        completed_brands.append({'Brand': brand, 'Page_Number': page_num})\n",
    "        save_progress(completed_brands)  \n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    df = pd.read_csv('Brand_Name.csv')\n",
    "    brand_names = df['Brand_ENG'].tolist()\n",
    "    last_completed_brand, start_page = load_progress()\n",
    "\n",
    "    for brand in tqdm(brand_names, desc=\"Overall Progress\"):\n",
    "        if brand == last_completed_brand:\n",
    "            start_page += 1  # 마지막으로 완료된 브랜드의 다음 페이지부터 시작\n",
    "        else:\n",
    "            start_page = 1  # 새 브랜드는 첫 페이지부터 시작\n",
    "        print(f\"Starting scraping for brand: {brand} from page {start_page}\")\n",
    "        scrape_brand_data(driver, brand, start_page=start_page)\n",
    "        start_page = 1  # 다음 브랜드를 위해 시작 페이지를 리셋\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaco0304",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
